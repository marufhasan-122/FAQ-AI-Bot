{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "faq_data = [\n",
        "  {\"question\": \"What is AI?\", \"answer\": \"AI is the simulation of human intelligence in machines.\"},\n",
        "  {\"question\": \"What is Machine Learning?\", \"answer\": \"Machine learning is a subset of AI where systems learn from data.\"},\n",
        "  {\"question\": \"What is Deep Learning?\", \"answer\": \"Deep learning is a subset of ML using neural networks with many layers.\"},\n",
        "  {\"question\": \"What is Natural Language Processing?\", \"answer\": \"NLP is a field of AI focused on interaction between computers and human language.\"},\n",
        "  {\"question\": \"What is Computer Vision?\", \"answer\": \"Computer vision enables machines to interpret and process visual information.\"},\n",
        "  {\"question\": \"What is Reinforcement Learning?\", \"answer\": \"Reinforcement learning is a type of ML where agents learn through trial and error.\"},\n",
        "  {\"question\": \"What is supervised learning?\", \"answer\": \"Supervised learning is training a model on labeled data.\"},\n",
        "  {\"question\": \"What is unsupervised learning?\", \"answer\": \"Unsupervised learning finds patterns in unlabeled data.\"},\n",
        "  {\"question\": \"What is a neural network?\", \"answer\": \"A neural network is a model inspired by the human brain, used to recognize patterns.\"},\n",
        "  {\"question\": \"What is the difference between AI and ML?\", \"answer\": \"AI is the broader concept, while ML is a subset of AI.\"}\n",
        "]\n",
        "\n",
        "with open(\"faq.json\", \"w\") as f:\n",
        "    json.dump(faq_data, f, indent=2)\n",
        "\n",
        "print(\"faq.json ✅\")\n",
        "\n"
      ],
      "metadata": {
        "id": "cfotuKmP8ljW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1da1bb7-755f-4e51-a38c-f04aed90bc52"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "faq.json ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report: In-Context Learning for FAQ Bot"
      ],
      "metadata": {
        "id": "9DI31FmeAKIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!ollama pull mistral\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgyBBOj5923V",
        "outputId": "0dea5bde-fb73-4526-8252-eac5ffd6b32a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Error: ollama server not responding - could not connect to ollama server, run 'ollama serve' to start it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"faq.json\", \"r\") as f:\n",
        "    faq_data = json.load(f)\n",
        "\n",
        "faq_data[:3]  # show first 3 entries\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xezG0XGR-FWF",
        "outputId": "de273799-c6a8-42bb-f437-acc5e4556152"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'question': 'What is AI?',\n",
              "  'answer': 'AI is the simulation of human intelligence in machines.'},\n",
              " {'question': 'What is Machine Learning?',\n",
              "  'answer': 'Machine learning is a subset of AI where systems learn from data.'},\n",
              " {'question': 'What is Deep Learning?',\n",
              "  'answer': 'Deep learning is a subset of ML using neural networks with many layers.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_context(faq_data, user_query):\n",
        "    context = \"You are an AI FAQ assistant. Use the dataset below to answer questions.\\n\\n\"\n",
        "    for item in faq_data:\n",
        "        context += f\"Q: {item['question']}\\nA: {item['answer']}\\n\\n\"\n",
        "    context += f\"User: {user_query}\\nAssistant:\"\n",
        "    return context\n"
      ],
      "metadata": {
        "id": "ln8NUuCM-KHd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import json\n",
        "\n",
        "def ask_ollama(user_query, faq_data):\n",
        "    prompt = build_context(faq_data, user_query)\n",
        "\n",
        "    # Run Ollama with subprocess\n",
        "    result = subprocess.run(\n",
        "        [\"ollama\", \"run\", \"mistral\"],\n",
        "        input=prompt.encode(\"utf-8\"),\n",
        "        capture_output=True,\n",
        "    )\n",
        "    return result.stdout.decode(\"utf-8\").strip()\n"
      ],
      "metadata": {
        "id": "dmXgLvJT-Oru"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def faq_bot(user_query, faq_data):\n",
        "    # simple match check\n",
        "    for item in faq_data:\n",
        "        if user_query.lower() in item['question'].lower():\n",
        "            return item['answer']\n",
        "\n",
        "    # otherwise, use Ollama to guess\n",
        "    response = ask_ollama(user_query, faq_data)\n",
        "\n",
        "    if response.strip() == \"\" or \"I don’t know\" in response:\n",
        "        return \"I don’t know, please refine your question.\"\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "asWaBKxV-RY2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_query = input(\"Ask a question (or type 'exit' to quit): \")\n",
        "    if user_query.lower() == \"exit\":\n",
        "        break\n",
        "    print(\"Bot:\", faq_bot(user_query, faq_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZb67pT1-Vet",
        "outputId": "465f6eca-e3ce-4be0-e806-e35be4dd74b8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask a question (or type 'exit' to quit): What is Reinforcement learning\n",
            "Bot: Reinforcement learning is a type of ML where agents learn through trial and error.\n",
            "Ask a question (or type 'exit' to quit): Explain details\n",
            "Bot: I don’t know, please refine your question.\n",
            "Ask a question (or type 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How In-Context Learning was Applied\n",
        "In this project, **In-Context Learning (ICL)** was applied by directly providing the FAQ dataset (question–answer pairs) inside the model’s prompt.  \n",
        "Instead of training or fine-tuning the model, the dataset was injected into the context, so whenever a user asks a question, the model uses the given context to generate the most relevant answer.  \n",
        "This approach makes the model act like it has been “temporarily trained” on our FAQ dataset without modifying its internal parameters."
      ],
      "metadata": {
        "id": "tjBK8QepASUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Limitations Compared to Fine-Tuning\n",
        "- **Context size limit**: The entire FAQ dataset must fit into the model’s input prompt. Large datasets cannot be handled effectively.\n",
        "- **No memory retention**: The model “forgets” the dataset once the session ends; unlike fine-tuning, it doesn’t permanently learn the knowledge.\n",
        "- **Less accurate matching**: The model may sometimes generate approximate or unrelated answers if the question does not closely resemble the dataset.\n",
        "- **Computation cost**: Each query requires passing the entire dataset into the prompt, which is inefficient compared to a fine-tuned model."
      ],
      "metadata": {
        "id": "KWs2qpRVAfv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, ICL is quick and flexible for small datasets, but fine-tuning is better for large, specialized FAQ systems."
      ],
      "metadata": {
        "id": "S5fvQ45gAj_W"
      }
    }
  ]
}